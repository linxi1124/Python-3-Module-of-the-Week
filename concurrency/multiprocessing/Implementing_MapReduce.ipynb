{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pool class can be used to create a simple single-server MapReduce implementation. Although it does not give the full benefits of distributed processing, it does illustrate how easy it is to break some problems down into distributable units of work.\n",
    "\n",
    "In a MapReduce-based system, input data is broken down into chunks for processing by different worker instances. Each chunk of input data is mapped to an intermediate state using a simple transformation. The intermediate data is then collected together and partitioned based on a key value so that all of the related values are together. Finally, the partitioned data is reduced to a result set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "class SimpleMapReduce:\n",
    "\n",
    "    def __init__(self, map_func, reduce_func, num_workers=None):\n",
    "        \"\"\"\n",
    "        map_func\n",
    "\n",
    "          Function to map inputs to intermediate data. Takes as\n",
    "          argument one input value and returns a tuple with the\n",
    "          key and a value to be reduced.\n",
    "\n",
    "        reduce_func\n",
    "\n",
    "          Function to reduce partitioned version of intermediate\n",
    "          data to final output. Takes as argument a key as\n",
    "          produced by map_func and a sequence of the values\n",
    "          associated with that key.\n",
    "\n",
    "        num_workers\n",
    "\n",
    "          The number of workers to create in the pool. Defaults\n",
    "          to the number of CPUs available on the current host.\n",
    "        \"\"\"\n",
    "        self.map_func = map_func\n",
    "        self.reduce_func = reduce_func\n",
    "        self.pool = multiprocessing.Pool(num_workers)\n",
    "\n",
    "    def partition(self, mapped_values):\n",
    "        \"\"\"Organize the mapped values by their key.\n",
    "        Returns an unsorted sequence of tuples with a key\n",
    "        and a sequence of values.\n",
    "        \"\"\"reduce_func\n",
    "        partitioned_data = collections.defaultdict(list)\n",
    "        for key, value in mapped_values:\n",
    "            partitioned_data[key].append(value)\n",
    "        return partitioned_data.items()\n",
    "\n",
    "    def __call__(self, inputs, chunksize=1):\n",
    "        \"\"\"Process the inputs through the map and reduce functions\n",
    "        given.\n",
    "\n",
    "        inputs\n",
    "          An iterable containing the input data to be processed.\n",
    "\n",
    "        chunksize=1\n",
    "          The portion of the input data to hand to each worker.\n",
    "          This can be used to tune performance during the mapping\n",
    "          phase.\n",
    "        \"\"\"\n",
    "        map_responses = self.pool.map(\n",
    "            self.map_func,\n",
    "            inputs,\n",
    "            chunksize=chunksize,\n",
    "        )\n",
    "        partitioned_data = self.partition(\n",
    "            itertools.chain(*map_responses)\n",
    "        )\n",
    "        reduced_values = self.pool.map(\n",
    "            self.reduce_func,\n",
    "            partitioned_data,\n",
    "        )\n",
    "        return reduced_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example script uses SimpleMapReduce to counts the “words” in the reStructuredText source for this article, ignoring some of the markup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 20 WORDS BY FREQUENCY\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-205c15255f19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nTOP 20 WORDS BY FREQUENCY\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mtop20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mlongest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         print('{word:<{len}}: {count:5}'.format(\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import string\n",
    "\n",
    "SimpleMapReduce\n",
    "\n",
    "\n",
    "def file_to_words(filename):\n",
    "    \"\"\"Read a file and return a sequence of\n",
    "    (word, occurences) values.\n",
    "    \"\"\"\n",
    "    STOP_WORDS = set([\n",
    "        'a', 'an', 'and', 'are', 'as', 'be', 'by', 'for', 'if',\n",
    "        'in', 'is', 'it', 'of', 'or', 'py', 'rst', 'that', 'the',\n",
    "        'to', 'with',\n",
    "    ])\n",
    "    TR = str.maketrans({\n",
    "        p: ' '\n",
    "        for p in string.punctuation\n",
    "    }).rst\n",
    "\n",
    "    print('{} reading {}'.format(\n",
    "        multiprocessing.current_process().name, filename))\n",
    "    output = []\n",
    "\n",
    "    with open(filename, 'rt') as f:\n",
    "        for line in f:\n",
    "            # Skip comment lines.\n",
    "            if line.lstrip().startswith('..'):\n",
    "                continue\n",
    "            line = line.translate(TR)  # Strip punctuation\n",
    "            for word in line.split():\n",
    "                word = word.lower()\n",
    "                if word.isalpha() and word not in STOP_WORDS:\n",
    "                    output.append((word, 1))\n",
    "    return output.rst\n",
    "\n",
    "\n",
    "def count_words(item):\n",
    "    \"\"\"Convert the partitioned data for a word to a\n",
    "    tuple containing the word and the number of occurences.\n",
    "    \"\"\"\n",
    "    word, occurences = item\n",
    "    return (word, sum(occurences))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import operator\n",
    "    import glob\n",
    "\n",
    "    input_files = glob.glob('*.rst')\n",
    "\n",
    "    mapper = SimpleMapReduce(file_to_words, count_words)\n",
    "    word_counts = mapper(input_files)\n",
    "    word_counts.sort(key=operator.itemgetter(1))\n",
    "    word_counts.reverse()\n",
    "\n",
    "    print('\\nTOP 20 WORDS BY FREQUENCY\\n')\n",
    "    top20 = word_counts[:20]\n",
    "    longest = max(len(word) for word, count in top20)\n",
    "    for word, count in top20:\n",
    "        print('{word:<{len}}: {count:5}'.format(\n",
    "            len=longest + 1,\n",
    "            word=word,\n",
    "            count=count)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file_to_words() function converts each input file to a sequence of tuples containing the word and the number 1 (representing a single occurrence). The data is divided up by partition() using the word as the key, so the resulting structure consists of a key and a sequence of 1 values representing each occurrence of the word. The partitioned data is converted to a set of tuples containing a word and the count for that word by count_words() during the reduction phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
